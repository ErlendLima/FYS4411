\section{Theory}\label{sec:Theory}
\subsection{Variational Monte Carlo}
In order to find a good candidate wavefunction for a given potential, one can
employ the \textit{variational principle}. One starts by guessing a trial
wavefunction \(\ket{\Psi_{T}}\) and estimating the trial energy, which is
guaranteed to be equal to or higher than the true ground state energy \(E_{0}\):
\begin{equation}
  \label{eq:1}
  E_{0} \le E = \frac{\ev{H}{\Psi_{T}}}{\ip{\Psi_{T}}}
\end{equation}

If \(\ket{\Psi_{T}}\) is an eigenfunction of the Hamiltonian, the variance
\(\sigma^{2}\) will be minimal

\begin{equation*}
  \sigma^{2} = \frac{\ev{H^{2}}{\Psi_{T}}}{\ip{\Psi_{T}}} -\left( \frac{\ev{H}{\Psi_{T}}}{\ip{\Psi_{T}}} \right)^{2} = 0
\end{equation*}

The variational principle expands on this idea by letting \(\ket{\Psi_{T}}\) be
a functional class of a \textit{variational parameter} \(\alpha\). By varying
\(\alpha\) one can find the optimal trial wavefunction within the functional
class by minimizing \(\sigma^{2}\). 

Only a small collection of potentials have analytical solution using the
variational principle. For most potentials, one must numerically
integrate~\eqref{eq:1} using Monte Carlo integration.

For a stochastic variable \(x\) with probability density function \(p(x)\), the
average \(\left< x \right>\) is defined as
\begin{equation*}
  \left< x \right> = \int_{\mathbb{R}} xp(x)\dd x
\end{equation*}
By sampling the stochastic variable \(M\) times, the average can be approximated
by 

\begin{equation*}
  \expval{x} = \int_{\mathbb{R}} xp(x)\dd x \approx \frac{1}{M}\sum_{i=1}^{M}x_{i}p(x_{i})
\end{equation*}

Applying this to an observable \(\mathcal{O}\), we have

\begin{align*}
  \expval{\mathcal{O}} &= \ev{\mathcal{O}}{\Psi}\\
                       &= \int \dd \vb{r} \Psi^{*}\mathcal{O}\Psi \\
                       &= \int \dd \vb{r} \abs{\Psi}^{2} \frac{1}{\Psi}\mathcal{O}\Psi\\
  &= \frac{1}{M} \sum_{i=1}^{M}p(\vb{r})\mathcal{O}_{L}
\end{align*}

where \(\abs{\Psi}^{2}\) is defined as the probability density function, and
\(\frac{1}{\Psi}\mathcal{O}\Psi\) the \textit{local operator}.

The local trial energy can then be defined as
\begin{equation*}
  E_{L} =\frac{1}{\Psi_{T}}H\Psi_{T}
\end{equation*}
which can be computed using Monte Carlo integration as

\begin{align*}
  \expval{E_{L}} \approx \frac{1}{M}\sum_{i=1}^{M} p(\vb{r}_{i})E_{L}(\vb{r}_{i})
\end{align*}

The goal is therefore to minimize minimizing \(\sigma^{2} = \expval{E_{L}^{2}} -
\expval{E_{L}}^{2}\) over the variational parameter \(\alpha\).

\subsection{Gradient Descent}
The optimal value for the variational parameter is found by gradient descent.

\subsection{The System}
\subsubsection{The Potentials}

The Hamiltonian under investigation describes \(N\) bosons in a potential trap,
and is on the form

\begin{equation*}
  H = \sum_{i=1}^{N}\left( \frac{-\hbar^{2}}{2m}\nabla^{2}_{i} + V_{\text{ext}}(\vb{r}_{i})\right) + \sum_{i<j}^{N}V_{\text{int}}(\vb{r}_{i}, \vb{r}_{j})
\end{equation*}

where \(V_{\text{ext}}\) is the external potential of the trap while
\(V_{\text{int}}\) is the internal potential between the particles. 
The external potential has an elliptical form, being anisotropic in the \(z\)-direction:

\begin{equation}
  \label{eq:2}
  V_{\text{ext}}(\vb{r}) = \frac{1}{2}m\left( \omega\left[ x^{2} + y^{2} \right] + \omega_{z}z^{2} \right)
\end{equation}

The internal potential is a hard shell potential, being infinite for distances
where two bosons overlap:

\begin{equation*}
  V_{\text{int}} = \begin{cases}
    \infty, &\text{for } |r_{i} - r_{j}| \le 0\\
    0, &\text{otherwise}
    \end{cases}
\end{equation*}

\subsubsection{The Trial Wavefunction}

The elliptical spherical trap~\eqref{eq:2} represents a harmonic oscillator. As
the trial wavefunction should be as close as possible to the expected true
wavefunction, a reasonable guess at its shape is the eigenfunction of
harmonic oscillators, namely Gaussian functions. For a \(N\)-bosonic system the
trial wavefunction is therefore

\begin{align*}
  h(\vb{r}_{1}, \ldots, \vb{r}_{N}, \alpha, \beta) &= \prod_{i=1}^{N}g(\vb{r}_{i}, \alpha, \beta)\\
  &= \exp{-\sum_{i=1}^{N}\left( x_{i}^{2} + y_{i}^{2}+\beta z_{i}^{2} \right)}
\end{align*}
with \(g\) the onebody function.
The internal potential should cause the wavefunction to decrease continuously
down to zero as the distance of two particles goes to zero. Once such possible
function is

\begin{equation*}
  f(a, \vb{r}_{i}, \vb{r}_{j}) = \begin{cases}
    0, &|\vb{r}_{i} - \vb{r}_{j}| \le a\\
    1 - \frac{a}{\abs{r_{i}-r_{j}}}, &\text{otherwise}
    \end{cases}
\end{equation*}

Combining both potential contributions, the complete trial wave function is
therefore

\begin{equation*}
  \Psi_{T}(\vb{r}, \alpha, \beta, a) = \exp{-\alpha\sum_{i=1}^{N}\left( x_{i}^{2} + y^{2}_{i} + \beta z^{2}_{i} \right)}
  \prod_{i<j}^{N}f(a, \vb{r}_{i}, \vb{r}_{j})
\end{equation*}

\subsubsection{Non-interacting Case}
For non-interacting bosons in a spherical with \(\beta = 1\) and \(a = 0\) the
system reduces to spherical harmonic oscillators where analytical solutions are
available. The trial wavefunction reduces to simply the product of one body
elements

\newcommand{\psit}{\Psi_T(\vb{r})}
\newcommand{\onebody}{\prod_{i}^{N}\exp{-\alpha\left[\left( x_i^2 + y_i^2 + \beta
      z_i^2\right)\right]}}
\begin{align*}
  \psit = \prod_{i}^{N}\exp[-\alpha\left( x_{i}^{2}+y_{i}^{2}+z_{i}^{2} \right)] = \prod_{i}^{N}\exp(-\alpha \abs{r_{i}}^{2})
\end{align*}

while the Hamiltonian reduces to

\begin{align*}
  H = \sum_{i}^{N} \frac{-\hbar^{2}}{2m}\nabla_{i}^{2} + \frac{1}{2}m\omega^{2}r_{i}^{2}
\end{align*}

which in natural units is

\newcommand{\lapl}[1]{\nabla_{#1}^2}
\begin{align*}
  H = \frac{1}{2}\sum_{i}^{N} -\frac{1}{m}\lapl{i} + m\omega^{2}r_{i}^{2}
\end{align*}

Applying the Hamiltonian gives the local energy as

\begin{align*}
  E_{L} = \frac{\alpha d}{m} N + \left( \frac{1}{2}m\omega^{2} - \frac{2\alpha^{2}}{m} \right)\sum_{i}^{N}r_{i}^{2}
\end{align*}
 where \(d\) is the dimension. As the factor \(\sum r_{i}^{2}\) is always
 positive, its term should be minimized, which is accomplished by setting
 \(\alpha = \frac{m\omega}{2}\), giving a minimal local energy of

 \begin{align*}
   E_{L} = \frac{\omega d N}{2}
   \end{align*}

\subsubsection{Interacting Case}
The local energy for the full interacting case is much more complicated. The
full computation is deferred to appendix~\ref{appendix:le}.





\subsection{Drift Force}
\subsection{Onebody Density}

\subsection{Blocking}
\label{sec:blocking}
When dealing with estimation based on data, either produced experimentally or numerically, a proper statistical treatment is appropriate. 

Perhaps the most common estimator in statistics is the mean:

\begin{equation}
	\label{eq:mean}
	\bar{X} = \frac{1}{N}\sum_{i=1}^{N}{X_i},
\end{equation}
where $X_i$ are identically distributed random variables, but not necessarily independent. To establish the uncertainty of the estimator, it necessary to calculate $V(\bar{X})$. By using the linearity of the variance
\begin{equation}
\label{eq:estimator var}
\begin{split}
	V(\bar{X}) = V(\frac{1}{N}\sum_{i=1}^{N}{X_i}) = \frac{1}{N^2}V(\sum_{i=1}^{N}{X_i})\\
	=\frac{1}{N^2}\sum_{i,j}{Cov(X_i, X_j)},
\end{split}
\end{equation}

where $Cov(X_i, X_j)$ is the covariance of $X_i$, and $X_j$. Under the assumption that the variables are independent, which is often approximately true, $Cov(X_i, X_j) = 0$, for $i \neq j$. The variance of the mean is thus simplified to 
\begin{equation*}
\begin{split}
	V(\bar{X}) = \frac{1}{N^2}\sum_{i}{Cov(X_i, X_i)} = \frac{1}{N^2}\sum_{i}{Var(X_i)} \\
	=\frac{N}{N^2}Var(X) = \frac{Var(X)}{N},
\end{split}
\end{equation*} 
where it has been used that the variables are identically distributed. This is a convenient result, as one can calculate the variance of estimator based on the variance of random variables $X$ that enter it. This is typically done using the sample variance
\begin{equation*}
	V(X) \approx \frac{1}{N}\sum_{i=1}^{N}{(x_i - \bar{x})^2},
\end{equation*}
where $x_i$ is some sample and $\bar{x} = 1/N\sum{x_i)}$ is the sample mean. Problems arise when the data is correlated, as the neglect of cross terms present in \autoref{eq:estimator var} causes a underestimation of the variance of the estimator. This is very important to bear in mind when working with VMC, as data produced by simulating many particles tends to be highly correlated.

Made popular by (kilde), blocking is a renormalization method for treating correlated data when calculating the variance of $\bar{X}$. Blocking is defined by a recursive downsampling of data. Starting with a vector containing the original data $\vec{X}_0$, blocking of degree $d$ is obtained by performing

\begin{equation*}
	(\vec{X}_{i+1})_k = \frac{1}{2}((\vec{X}_{i})_{2k -1} + (\vec{X}_{i})_{2k}),
\end{equation*}
for all $1 \leq i leq d-1$. 
\newline
It can be shown that the quantity 

\begin{equation*}
	V(\bar{X_k}) \approx \frac{\hat{\sigma}_k^2}{n_k} +  e_k
\end{equation*}

is invariant with respect to the degree of blocking $k$, where $\hat{\sigma}_k^2$ is the sample variance of $\vec{X}_k$, and $e_k$ is know as the truncation error. Under the assumption that the original data $\vec{X}_0$ is asymptotically uncorrelated, (kilde) showed that $e_k$ is decreasing, and can be make small given big enough $k$. Therefore, for sufficiently large $k$

\begin{equation}\label{eq:trunc}
V(\bar{X}_0) = V(\bar{X}_k) \approx \frac{\hat{\sigma}_k^2}{n_k} +  e_k \approx \frac{\hat{\sigma}_k^2}{n_k}
\end{equation}

In this way, the sample variance of the sufficiently blocked data does not suffer from underestimation, as the correlation is first eliminated. 
  